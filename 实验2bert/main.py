import warnings
warnings.filterwarnings('ignore', category=FutureWarning)

import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import BertTokenizer
from config import Config
from dataset import SentimentDataset
from load_data import DataLoader as DataLoaderClass
from model import SentimentClassifier
import torch.nn as nn
import os

# æ–°å¢åº“ï¼ˆå¯è§†åŒ– & æŒ‡æ ‡ï¼‰
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix

os.environ['CUDA_LAUNCH_BLOCKING'] = '1'


# ============================================================
# è®¾ç½® HuggingFace é•œåƒ
# ============================================================
def set_hf_mirrors():
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    os.environ['HF_HOME'] = './hf_cache'

set_hf_mirrors()


# ============================================================
# ğŸ” è¯„ä¼°å‡½æ•°ï¼ˆæ–°å¢ï¼šF1ã€AUCã€Confusion Matrixï¼‰
# ============================================================
def evaluate(model, eval_loader, device):
    model.eval()
    total_loss = 0
    all_labels = []
    all_preds = []
    all_probs = []

    with torch.no_grad():
        for batch in eval_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask)
            loss = nn.CrossEntropyLoss()(outputs, labels)
            total_loss += loss.item()

            probs = torch.softmax(outputs, dim=1)[:, 1]  # æ­£ç±»æ¦‚ç‡
            _, preds = torch.max(outputs, dim=1)

            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    # ===== æŒ‡æ ‡è®¡ç®— =====
    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)

    try:
        auc = roc_auc_score(all_labels, all_probs)
    except:
        auc = 0.0

    cm = confusion_matrix(all_labels, all_preds)

    avg_loss = total_loss / len(eval_loader)
    return avg_loss, acc, f1, auc, cm


# ============================================================
# ğŸ“ˆ å¯è§†åŒ–: Loss / Accuracy / F1 / AUC
# ============================================================
def plot_training_curves(history):
    epochs = range(1, len(history["train_loss"]) + 1)

    # ---- Loss æ›²çº¿ ----
    plt.figure(figsize=(6,4))
    plt.plot(epochs, history["train_loss"], label="Train Loss")
    plt.plot(epochs, history["val_loss"], label="Val Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Loss Curve")
    plt.ylim(bottom=0)  # yè½´ä»0å¼€å§‹
    plt.legend()
    plt.show()

    # ---- Accuracy ----
    plt.figure(figsize=(6,4))
    plt.plot(epochs, history["val_acc"], label="Val Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title("Validation Accuracy Curve")
    plt.ylim(bottom=0)  # yè½´ä»0å¼€å§‹
    plt.legend()
    plt.show()

    # ---- F1-score ----
    plt.figure(figsize=(6,4))
    plt.plot(epochs, history["val_f1"], label="Val F1-score")
    plt.xlabel("Epoch")
    plt.ylabel("F1-score")
    plt.title("F1-score Curve")
    plt.ylim(bottom=0)  # yè½´ä»0å¼€å§‹
    plt.legend()
    plt.show()

    # ---- AUC ----
    plt.figure(figsize=(6,4))
    plt.plot(epochs, history["val_auc"], label="Val AUC")
    plt.xlabel("Epoch")
    plt.ylabel("AUC")
    plt.title("AUC Curve")
    plt.ylim(bottom=0)  # yè½´ä»0å¼€å§‹
    plt.legend()
    plt.show()


    # ---- F1-score ----
    plt.figure(figsize=(6,4))
    plt.plot(epochs, history["val_f1"], label="Val F1-score")
    plt.xlabel("Epoch")
    plt.ylabel("F1")
    plt.title("F1-score Curve")
    plt.legend()
    plt.show()

    # ---- AUC ----
    plt.figure(figsize=(6,4))
    plt.plot(epochs, history["val_auc"], label="Val AUC")
    plt.xlabel("Epoch")
    plt.ylabel("AUC")
    plt.title("AUC Curve")
    plt.legend()
    plt.show()


# ============================================================
# ğŸ“Š æ··æ·†çŸ©é˜µå¯è§†åŒ–
# ============================================================
def plot_confusion_matrix(cm):
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title("Confusion Matrix")
    plt.show()


# ============================================================
# ğŸ”¥ è®­ç»ƒå‡½æ•°ï¼ˆæ–°å¢è®°å½•æŒ‡æ ‡ historyï¼‰
# ============================================================
def train(train_texts, train_labels, val_texts=None, val_labels=None):
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    config = Config()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    tokenizer = BertTokenizer.from_pretrained(config.model_name)
    model = SentimentClassifier(config.model_name, config.num_classes)
    model.to(device)

    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, config.max_seq_length)
    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)

    val_loader = None
    if val_texts:
        val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, config.max_seq_length)
        val_loader = DataLoader(val_dataset, batch_size=config.batch_size)

    optimizer = AdamW(model.parameters(), lr=config.learning_rate)

    # ===== æ–°å¢ï¼šè®°å½•è®­ç»ƒè¿‡ç¨‹ =====
    history = {
        "train_loss": [],
        "val_loss": [],
        "val_acc": [],
        "val_f1": [],
        "val_auc": []
    }

    best_acc = 0

    for epoch in range(config.num_epochs):
        model.train()
        total_loss = 0

        for batch in train_loader:
            optimizer.zero_grad()

            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask)
            loss = nn.CrossEntropyLoss()(outputs, labels)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)
        print(f"\nEpoch {epoch+1}/{config.num_epochs}")
        print(f"Train Loss: {avg_train_loss:.4f}")

        if val_loader:
            val_loss, acc, f1, auc, cm = evaluate(model, val_loader, device)

            print(f"Val Loss: {val_loss:.4f}")
            print(f"Accuracy: {acc:.4f}")
            print(f"F1-score: {f1:.4f}")
            print(f"AUC: {auc:.4f}")
            print("Confusion Matrix:")
            print(cm)

            history["train_loss"].append(avg_train_loss)
            history["val_loss"].append(val_loss)
            history["val_acc"].append(acc)
            history["val_f1"].append(f1)
            history["val_auc"].append(auc)

            if acc > best_acc:
                best_acc = acc
                model.save_model(config.model_save_path)
                print("âœ” Best model updated.")

    return model, history, tokenizer, device


# ============================================================
# ğŸš€ ä¸»ç¨‹åºå…¥å£
# ============================================================
if __name__ == "__main__":
    config = Config()
    loader = DataLoaderClass(config)

    train_texts, train_labels = loader.load_csv("dataset/train_2000.csv")
    val_texts, val_labels = loader.load_csv("dataset/dev.csv")
    test_texts, test_labels = loader.load_csv("dataset/test.csv")

    # ===== è®­ç»ƒ =====
    model, history, tokenizer, device = train(train_texts, train_labels, val_texts, val_labels)

    # ===== å¯è§†åŒ– =====
    plot_training_curves(history)

    # ===== æµ‹è¯•é›†è¯„ä¼° =====
    print("\n========== Testing ==========")
    test_dataset = SentimentDataset(test_texts, test_labels, tokenizer, config.max_seq_length)
    test_loader = DataLoader(test_dataset, batch_size=config.batch_size)

    model = SentimentClassifier(config.model_name, config.num_classes)
    model.load_state_dict(torch.load(config.model_save_path))
    model.to(device)

    test_loss, acc, f1, auc, cm = evaluate(model, test_loader, device)

    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Accuracy: {acc:.4f}")
    print(f"Test F1-score: {f1:.4f}")
    print(f"Test AUC: {auc:.4f}")
    print("Test Confusion Matrix:")
    print(cm)

    plot_confusion_matrix(cm)
